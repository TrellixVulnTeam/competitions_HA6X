{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_naive = Path(\"../input/birdclef-2021-naive-npy\")\n",
    "path_naive.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shortaudio_train = pd.read_csv(path_naive/\"shortaudio_train.csv\")\n",
    "df_shortaudio_val = pd.read_csv(path_naive/\"shortaudio_val.csv\")\n",
    "\n",
    "df_soundscape_train = pd.read_csv(path_naive/\"soundscape_train.csv\")\n",
    "df_soundscape_val = pd.read_csv(path_naive/\"soundscape_val.csv\")\n",
    "df_soundscape_test = pd.read_csv(path_naive/\"soundscape_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_soundscape_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shortaudio_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have forgotten to add `month_x`, etc. to `shortaudio_{train,val,test}.csv`. Let's make that up.\n",
    "\n",
    "This is not necessarily a bad thing -- By forgetting this, our `.csv` files are more ligth-weighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyclicize_number(number, max_, min_):\n",
    "    \"\"\"\n",
    "    args\n",
    "        number, int\n",
    "            \\in {min_, min_ + 1, ..., max_}\n",
    "            e.g. hour => min_ = 0, max_ = 24\n",
    "                 longitude => min_ = -180, max_ = 180\n",
    "        max_, int\n",
    "        min_, int\n",
    "    return\n",
    "        (x, y), tuple of float\n",
    "    \"\"\"\n",
    "    period = max_ - min_\n",
    "    theta = 2 * np.pi * (number / period)\n",
    "    #theta = 2 * np.pi * ((number - min_) / period)\n",
    "    x = np.cos(theta)\n",
    "    y = np.sin(theta)\n",
    "    return x, y\n",
    "\n",
    "# N.B. Using the next function to deal with df_train_soundscape is\n",
    "#      not efficient, since there are only 4 distinct longitudes.\n",
    "def cyclicize_series(series, max_, min_):\n",
    "    return list(map(lambda number: cyclicize_number(number, max_, min_), series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shortaudio_train[[\"month_x\", \"month_y\"]] = cyclicize_series(df_shortaudio_train[\"month\"], 12, 0)\n",
    "df_shortaudio_train[[\"day_coarse_x\", \"day_coarse_y\"]] = cyclicize_series(df_shortaudio_train[\"day\"], 31, 0)\n",
    "df_shortaudio_train[[\"longitude_x\", \"longitude_y\"]] = cyclicize_series(df_shortaudio_train[\"longitude\"], 180, -180)\n",
    "df_shortaudio_train[\"latitude_normalized\"] = df_shortaudio_train[\"latitude\"] / 90\n",
    "df_shortaudio_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shortaudio_val[[\"month_x\", \"month_y\"]] = cyclicize_series(df_shortaudio_val[\"month\"], 12, 0)\n",
    "df_shortaudio_val[[\"day_coarse_x\", \"day_coarse_y\"]] = cyclicize_series(df_shortaudio_val[\"day\"], 31, 0)\n",
    "df_shortaudio_val[[\"longitude_x\", \"longitude_y\"]] = cyclicize_series(df_shortaudio_val[\"longitude\"], 180, -180)\n",
    "df_shortaudio_val[\"latitude_normalized\"] = df_shortaudio_val[\"latitude\"] / 90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Produce common columns for the two diff types of dataframes\n",
    "- Build `df_train`, `df_val`, `df_test`\n",
    "- Build `XX_train`, `XX_val`, `XX_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_feature_columns = [\n",
    "    \"month_x\",\n",
    "    \"month_y\",\n",
    "    \"day_coarse_x\",\n",
    "    \"day_coarse_y\",\n",
    "    \"longitude_x\",\n",
    "    \"longitude_y\",\n",
    "    \"latitude_normalized\",\n",
    "]\n",
    "\n",
    "L_common_columns = L_feature_columns + [\n",
    "    \"npy_filename\",\n",
    "    \"primary_label\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_soundscape_train[\"npy_filename\"] = df_soundscape_train[\"row_id\"] + \".npy\"\n",
    "df_soundscape_val[\"npy_filename\"] = df_soundscape_val[\"row_id\"] + \".npy\"\n",
    "df_soundscape_test[\"npy_filename\"] = df_soundscape_test[\"row_id\"] + \".npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_soundscape_train.rename(\n",
    "    {\"birds\": \"primary_label\"},\n",
    "    axis=\"columns\",\n",
    "    inplace=True,\n",
    ")\n",
    "df_soundscape_val.rename(\n",
    "    {\"birds\": \"primary_label\"},\n",
    "    axis=\"columns\",\n",
    "    inplace=True,\n",
    ")\n",
    "df_soundscape_test.rename(\n",
    "    {\"birds\": \"primary_label\"},\n",
    "    axis=\"columns\",\n",
    "    inplace=True,\n",
    ")\n",
    "\"primary_label\" in df_soundscape_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([\n",
    "    df_shortaudio_train[L_common_columns],\n",
    "    df_soundscape_train[L_common_columns],\n",
    "])\n",
    "df_train.shape, df_shortaudio_train.shape, df_soundscape_train.shape, len(L_common_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = pd.concat([\n",
    "    df_shortaudio_val[L_common_columns],\n",
    "    df_soundscape_val[L_common_columns],\n",
    "])\n",
    "df_val.shape, df_shortaudio_val.shape, df_soundscape_val.shape, len(L_common_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_soundscape_test[L_common_columns]\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe we need to shuffle before assigning `df_train[L_feature_columns].value` to `XX_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.sample(frac=1)\n",
    "df_val = df_val.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX_train = df_train[L_feature_columns].values\n",
    "XX_val = df_val[L_feature_columns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Generator\n",
    "Why switch to using generator? How large is our data this time? Could you make an estimate?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    npy_filename_i = df_train[\"npy_filename\"].iloc[i]\n",
    "    print(npy_filename_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"npy_filename\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX_train.shape, df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX_train[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX_train[100].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATASET = Path(\"../input/birdclef-2021\")\n",
    "\n",
    "L_birds = [path.name for path\n",
    "           in (PATH_DATASET / \"train_short_audio\").iterdir()]\n",
    "L_birds = sorted(L_birds)\n",
    "D_label_index = {label: i for i, label in enumerate(L_birds)}\n",
    "D_index_label = {v: k for k, v in D_label_index.items()}\n",
    "\n",
    "def label(series):\n",
    "    #I = np.eye(len(D_label_index))\n",
    "    y = np.zeros((len(series), len(D_label_index)), dtype=np.float32)\n",
    "    for i, string in enumerate(series.values):\n",
    "    #for i, string in enumerate(series):\n",
    "        if string == \"nocall\":\n",
    "            continue\n",
    "        else:\n",
    "            L_indices = [D_label_index[label] for label in string.split(\" \")]\n",
    "            #row_i = np.sum(I[L_indices], axis=0)\n",
    "            #y[i] = row_i\n",
    "            y[i, L_indices] = 1\n",
    "    return y\n",
    "\n",
    "y_train = label(df_train[\"primary_label\"])\n",
    "np.unique(np.sum(y_train, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = label(df_val[\"primary_label\"])\n",
    "#y_test = label(df_test[\"primary_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.choice(list((path_naive / \"train_npy\").iterdir()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_npy_path = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_npy = np.load(random_npy_path)\n",
    "random_npy.dtype, random_npy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w = random_npy.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-23T02:29:06.759892Z",
     "iopub.status.busy": "2021-05-23T02:29:06.759348Z",
     "iopub.status.idle": "2021-05-23T02:29:06.767716Z",
     "shell.execute_reply": "2021-05-23T02:29:06.76708Z",
     "shell.execute_reply.started": "2021-05-23T02:29:06.759857Z"
    }
   },
   "source": [
    "def trainset_generator():\n",
    "    for i in range(df_train.shape[0]):\n",
    "        npy_filename_i = df_train[\"npy_filename\"].iloc[i]\n",
    "        image_i = np.load(path_naive / f\"train_npy/{npy_filename_i}\").astype(np.float32, copy=False)\n",
    "        image_i /= 255.0\n",
    "        image_i = np.repeat(image_i[..., np.newaxis], 3, axis=-1)  # shape: (h, w, 3)\n",
    "        features_i = XX_train[i]                                  # shape: (7,)\n",
    "        y_i = y_train[i]                                          # shape: (475238, 397)\n",
    "        yield (image_i, features_i), y_i\n",
    "        #yield image_i, features_i, y_i\n",
    "\n",
    "def valset_generator():\n",
    "    for i in range(df_val.shape[0]):\n",
    "        npy_filename_i = df_val[\"npy_filename\"].iloc[i]\n",
    "        image_i = np.load(path_naive / f\"val_npy/{npy_filename_i}\").astype(np.float32, copy=False)\n",
    "        image_i /= 255.0\n",
    "        image_i = np.repeat(image_i[..., np.newaxis], 3, axis=-1)  # shape: (h, w, 3)\n",
    "        features_i = XX_val[i]                                    # shape: (7,)\n",
    "        y_i = y_val[i]\n",
    "        yield (image_i, features_i), y_i\n",
    "        #yield image_i, features_i, y_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "class DatasetGenerator(Sequence):\n",
    "    def __init__(self, df, XX, y, h, w, batch_size=32):\n",
    "        self.df = df\n",
    "        self.XX = XX\n",
    "        self.y = y\n",
    "        self.h = h\n",
    "        self.w = w\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        #return self.df.shape[0]\n",
    "        return self.df.shape[0] // self.batch_size\n",
    "\n",
    "    #def image_processing(i, batch_npy_filename, batch_image):\n",
    "    #    npy_filename_i = batch_npy_filename.iloc[i]\n",
    "    #    image_i = np.load(path_naive / f\"train_npy/{npy_filename_i}\").astype(np.float32, copy=False)\n",
    "    #    image_i /= 255.0\n",
    "    #    image_i = np.repeat(image_i[..., np.newaxis], 3, axis=-1)  # shape: (h, w, 3)\n",
    "    #    batch_image[i] = image_i\n",
    "    def image_processing(self, i):\n",
    "        npy_filename_i = self.batch_npy_filename.iloc[i]\n",
    "        image_i = np.load(path_naive / f\"train_npy/{npy_filename_i}\").astype(np.float32, copy=False)\n",
    "        image_i /= 255.0\n",
    "        image_i = np.repeat(image_i[..., np.newaxis], 3, axis=-1)  # shape: (h, w, 3)\n",
    "        self.batch_image[i] = image_i\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self.batch_npy_filename = self.df[\"npy_filename\"].iloc[idx*self.batch_size: (idx + 1)*self.batch_size]\n",
    "        #batch_image = np.empty((self.batch_size, self.h, self.w, 3), dtype=np.float32)\n",
    "        self.batch_image = np.zeros((self.batch_size, self.h, self.w, 3), dtype=np.float32)\n",
    "\n",
    "        ## joblib, multiprocessing\n",
    "        #tasks = [delayed(self.image_processing)(i) for i in range(self.batch_size)]\n",
    "        #pool = Parallel(n_jobs=8)\n",
    "        #pool(tasks)\n",
    "        \n",
    "        ## single-core implementation\n",
    "        for i in range(self.batch_size):\n",
    "            self.image_processing(i)\n",
    "        batch_X = [self.batch_image, self.XX[idx*self.batch_size: (idx + 1)*self.batch_size]]\n",
    "        batch_y = self.y[idx*self.batch_size: (idx + 1)*self.batch_size]\n",
    "        return batch_X, batch_y\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "class DatasetGenerator(Sequence):\n",
    "    def __init__(self, df, XX, y, h, w, is_train=True, batch_size=32):\n",
    "        self.df = df\n",
    "        self.XX = XX\n",
    "        self.y = y\n",
    "        self.h = h\n",
    "        self.w = w\n",
    "        self.is_train = is_train\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        #return self.df.shape[0]\n",
    "        return self.df.shape[0] // self.batch_size\n",
    "\n",
    "    #def image_processing(i, batch_npy_filename, batch_image):\n",
    "    #    npy_filename_i = batch_npy_filename.iloc[i]\n",
    "    #    image_i = np.load(path_naive / f\"train_npy/{npy_filename_i}\").astype(np.float32, copy=False)\n",
    "    #    image_i /= 255.0\n",
    "    #    image_i = np.repeat(image_i[..., np.newaxis], 3, axis=-1)  # shape: (h, w, 3)\n",
    "    #    batch_image[i] = image_i\n",
    "    def image_processing(self, i):\n",
    "        npy_filename_i = self.batch_npy_filename.iloc[i]\n",
    "        if self.is_train:\n",
    "            image_i = np.load(path_naive / f\"train_npy/{npy_filename_i}\").astype(np.float32, copy=False)\n",
    "        else:\n",
    "            image_i = np.load(path_naive / f\"val_npy/{npy_filename_i}\").astype(np.float32, copy=False)\n",
    "        image_i /= 255.0\n",
    "        image_i = np.repeat(image_i[..., np.newaxis], 3, axis=-1)  # shape: (h, w, 3)\n",
    "        self.batch_image[i] = image_i\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self.batch_npy_filename = self.df[\"npy_filename\"].iloc[idx*self.batch_size: (idx + 1)*self.batch_size]\n",
    "        #batch_image = np.empty((self.batch_size, self.h, self.w, 3), dtype=np.float32)\n",
    "        self.batch_image = np.zeros((self.batch_size, self.h, self.w, 3), dtype=np.float32)\n",
    "\n",
    "        ## joblib, multiprocessing\n",
    "        #tasks = [delayed(self.image_processing)(i) for i in range(self.batch_size)]\n",
    "        #pool = Parallel(n_jobs=8)\n",
    "        #pool(tasks)\n",
    "        \n",
    "        ## single-core implementation\n",
    "        for i in range(self.batch_size):\n",
    "            self.image_processing(i)\n",
    "        batch_X = [self.batch_image, self.XX[idx*self.batch_size: (idx + 1)*self.batch_size]]\n",
    "        batch_y = self.y[idx*self.batch_size: (idx + 1)*self.batch_size]\n",
    "        return batch_X, batch_y\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "for ((i, f), y) in DatasetGenerator(df_train, XX_train, y_train, h, w):\n",
    "    if k > 10:\n",
    "        break\n",
    "    print(i.shape)\n",
    "    print(f.shape)\n",
    "    print(y.shape)\n",
    "    k += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-23T02:21:30.85606Z",
     "iopub.status.busy": "2021-05-23T02:21:30.855664Z",
     "iopub.status.idle": "2021-05-23T02:21:30.87315Z",
     "shell.execute_reply": "2021-05-23T02:21:30.872016Z",
     "shell.execute_reply.started": "2021-05-23T02:21:30.856027Z"
    }
   },
   "source": [
    "help(keras.layers.Concatenate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_mels = keras.layers.Input(shape=(*random_npy.shape, 3), name=\"input_mels\")\n",
    "input_spacetime = keras.layers.Input(shape=(XX_train.shape[1],),\n",
    "                                     name=\"input_spacetime\")\n",
    "\n",
    "output_efficient = EfficientNetB0(include_top=False, weights=\"imagenet\")(input_mels)\n",
    "pooled = keras.layers.GlobalAveragePooling2D()(output_efficient)\n",
    "concatenated = keras.layers.Concatenate()([pooled, input_spacetime])\n",
    "#concatenated = keras.layers.concatenate([pooled, input_spacetime])\n",
    "#dropped = keras.layers.Dropout(.2)(pooled)\n",
    "dropped = keras.layers.Dropout(.2)(concatenated)\n",
    "output_CNN = keras.layers.Dense(len(L_birds), activation=\"sigmoid\")(dropped)\n",
    "model = keras.Model(\n",
    "    #inputs=[input_mels],\n",
    "    inputs=[input_mels, input_spacetime],\n",
    "    outputs=[output_CNN],\n",
    ")\n",
    "#model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"])\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[keras.metrics.Precision(), keras.metrics.Recall()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"model1.h5\",\n",
    "                                                save_best_only=True)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,\n",
    "                                                  restore_best_weights=True)\n",
    "\n",
    "EPSILON = 1e-6\n",
    "class PrintF1Score(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        #print(f\"logs.keys() = {logs.keys()}\")  # This can check what keys logs has.\n",
    "        f1_score = 2 * logs[\"precision\"] * logs[\"recall\"] / (logs[\"precision\"] + logs[\"recall\"] + EPSILON)\n",
    "        val_f1_score = 2 * logs[\"val_precision\"] * logs[\"val_recall\"] / (logs[\"val_precision\"] + logs[\"val_recall\"] + EPSILON)\n",
    "        print(f\"f1_score: {f1_score}\")\n",
    "        print(f\"val_f1_score: {val_f1_score}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-22T15:44:12.387141Z",
     "iopub.status.busy": "2021-05-22T15:44:12.38677Z",
     "iopub.status.idle": "2021-05-22T15:44:12.394027Z",
     "shell.execute_reply": "2021-05-22T15:44:12.39279Z",
     "shell.execute_reply.started": "2021-05-22T15:44:12.38711Z"
    }
   },
   "source": [
    "help(model.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    DatasetGenerator(df_train, XX_train, y_train, h, w),\n",
    "    batch_size=32,\n",
    "    epochs=100,\n",
    "    validation_data=DatasetGenerator(df_val, XX_val, y_val, h, w),\n",
    "    callbacks=[checkpoint_cb, early_stopping_cb, PrintF1Score()],\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ValueError: Input 0 of layer dense is incompatible with the layer: expected axis -1 of input shape to have value 1287 but received input with shape (1, 1281)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug\n",
    "The problem might be that the dataset generator should not have been rewritten this simply, i.e. using simple generator with `yield`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.layers.Concatenate()([np.zeros((32, 1280)), np.zeros((32, 7))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "for ((i, f), y) in trainset_generator():\n",
    "    if k > 10:\n",
    "        break\n",
    "    print(i.shape)\n",
    "    print(f.shape)\n",
    "    print(y.shape)\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "128 * 201 * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
